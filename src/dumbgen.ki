import dumbanal;
import state;

defenum cell_loc {
  // Directly via an frame offset, per se.
  DirectLoc i32;
  // Indirectly via a pointer (through the frame offset).
  IndirectLoc i32;
};

deftype lives struct {
  all hash[cell_num, void];
};

deftype gn_annot struct {
  // Which cells are guaranteed to be live when the gn starts.
  live nc[lives];

  // Only for GrApply and GrPrimApply.
  lower_paramlists nc[array[gr_num]];
  higher_paramlists hash[gr_num, void];
  paramlist_bottom_frame_offset nc[i32];

  // Some secsize(&f->text) value -- the jmp target for evaluating the node _and_ its precs.
  instruction_offset nc[u32];
};

deftype frame struct {
  cs *checkstate;
  im *identmap;
  argcells array[cell_num];
  return_cell cell_num;
  gr *frame_graph;
  bas *basic_analyze_state;
  // by_gn[i].fs shows what the state must is _before_ evaluating node i.  (All incoming nodes must agree!)
  by_gn array[gn_annot];
  // offsets[i] is the offset of a cell relative to the stack frame, or location of a cell, or something like that.
  offsets nc[array[cell_loc]];
  low_offset nc[i32];
  // An offset into &f->text of a 32-bit jmp offset, and the gn.
  placeholder_jump_fillin array[jmpdata];
  crash_jumps array[crash_jmpdata];
};

deftype jmpdata struct {
  overwrite_location u32;
  // On x86, this is overwrite_location + 4 for jmp and jcc instructions -- the location of the end of the instruction.
  relative_mathpoint u32;
  target gr_num;
};

deftype crash_jmpdata struct {
  overwrite_location u32;
  // Just a subset of jmpdata -- its target is hard-coded.
  relative_mathpoint u32;
};

func mk_gn_annot() gn_annot {
  return {NotComputed(), NotComputed(), mk_hash@[gr_num, void](), NotComputed(), NotComputed()};
}

func unIndirect(loc cell_loc) i32 {
  if case IndirectLoc(x i32) = loc {
    return x;
  } else {
    ice(_u8("unIndirect fail"));
    return fake();
  }
}

func un_Direct(loc cell_loc) i32 {
  if case DirectLoc(x i32) = loc {
    return x;
  } else {
    ice(_u8("un_Direct fail"));
    return fake();
  }
}

func get_loc(h *frame, c cell_num) cell_loc {
  return get(unHas(&h->offsets), c.~);
}

func get_ptr_loc(h *frame, c cell_num) cell_loc {
  check(ref_cell(h->gr, c)->props.flat_size == h->cs->plat.ptrtraits.size);
  return get_loc(h, c);
}

func get_size_loc(h *frame, c cell_num) cell_loc {
  check(ref_cell(h->gr, c)->props.flat_size == h->cs->plat.sizetraits.flat.size);
  return get_loc(h, c);
}

// Some Hungarian notation:  The "dumbly_" prefix means the code's really dumb.

func dumbly_gen_graphed_fn_body(cs *checkstate, f *objfile, bas *basic_analyze_state, g *graphed_fn_body) bool {
  h frame = {cs, cs->im, g->argcells, g->graph.cell, &g->graph.gr, bas, repeat(count(&g->graph.gr.ops), mk_gn_annot()), NotComputed(), NotComputed@[i32](), mk_array@[jmpdata](), mk_array@[crash_jmpdata]()};

  prelive lives;
  check_add_all(&prelive.all, &g->argcells);
  check_add(&prelive.all, g->graph.cell);

  DBG(_u8("gen graphed fn body for "), lookup(cs->im, g->graph.gr.informal_name));
  mut genexpr_result;
  if !dumbly_analyze_expression(&h, &g->graph.gr, g->graph.gn, &prelive, &mut) {
    return false;
  }

  switch &mut {
  case &Terminal(m exprmut):
    new_live lives = prelive;
    compose_live(&new_live, &m);
    expected lives = {mk_hash(g->graph.cell, void)};
    if !lives_equal(&new_live, &expected) {
      ice(_u8("fn_body leaves cells alive, expected "), g->graph.cell, _u8(", saw "), new_live);
    }
  case &NonTerminal(pm partial_exprmut):
    // nothing to assert
  }

  if !dumbly_layout_frame_cells(g, &h) {
    return false;
  }

  if !dumbly_gen_machine_code(f, &h, g->graph.gn) {
    return false;
  }

  return true;
}

func paramlist_cells(gr *frame_graph, gn gr_num) *shray[cell_num] {
  switch &ref_node(gr, gn)->op {
  case &GrApply(a gr_apply):
    return &a.params;
  case &GrPrimApply(a gr_prim_apply):
    return &a.params;
  default:
    ice(_u8("paramlist_cells called on non-paramlist op"));
    return fake();
  }
}

func dumbly_layout_frame_cells(g *graphed_fn_body, h *frame) bool {
  // Left-most paramlist goes first.
  paramlist_ordering array[gr_num];
  if !dumbly_linearize_paramlist_ordering(h, &paramlist_ordering) {
    return false;
  }

  // Offsets of either a static cell or a virtual cell's pointer.
  offsets array[opt[cell_loc]] = repeat(count(&h->gr->cells), None());

  // TODO: Maybe after we push the frame pointer, the alignment is 4 or something -- in cases where we need 8-byte or 16-byte alignment for some things.
  low_offset i32 = 0;

  // 0. Layout arg cells, return cell.
  switch h->cs->plat.callconv {
  case UniversalCdeclConvention(v void):
    // It's OK to hard-code numbers here, because we're under the bullshit "UniversalCDeclConvention".
    upward_offset i32 = 8;
    ret_ci *cell_info = ref_cell(h->gr, h->return_cell);
    if for_universal_cc_exists_hidden_return_param(&h->cs->plat, &ret_ci->props) {
      set(ref(&offsets, h->return_cell.~), IndirectLoc(upward_offset));
      upward_offset = upward_offset + 4;
    } else {
      // Put its cell below the frame pointer.
      offset cell_loc = next_stack_offset(h->cs, low_offset, ret_ci, &low_offset);
      set(ref(&offsets, h->return_cell.~), offset);
    }

    nargcells size = count(&h->argcells);
    for i size = 0; i < nargcells; i = i + 1 {
      c cell_num = get(&h->argcells, i);
      ci *cell_info = ref_cell(h->gr, c);
      if ci->location != LocationStatic(void) {
        ice(_u8("Non-static arglist cell"));
      }
      offset i32 = upward_offset;
      upward_offset = ceil_aligned(offset + ~ci->props.flat_size, 4);
      set(ref(&offsets, c.~), DirectLoc(offset));
    }
  }


  ncalls size = count(&paramlist_ordering);
  for i size = 0; i < ncalls; i = i + 1 {
    gn gr_num = get(&paramlist_ordering, i);
    annot *gn_annot = ref_annot(h, gn);
    live *lives = unHas(&annot->live);

    // 1. Layout any cells live during the funcall, that are _not_ part of the paramlist.
    it hash_iter[cell_num, void] = iter(&live->all);
    while case Has(p *tup[cell_num, void]) = next(&it) {
      c cell_num = p->car;
      if isNone(ref(&offsets, c.~)) {
        if case &Has(cell_paramlistgn gr_num) = &ref(&h->bas->celldisp, c.~)->paramlist {
          // Cells in other paramlists should already been processed -- and would have failed the isNone check just above.
          if cell_paramlistgn == gn {
            ice(_u8("cell is in other paramlist, was not processed"));
          }
        } else {
          ci *cell_info = ref_cell(h->gr, c);
          offset cell_loc = next_stack_offset(h->cs, low_offset, ci, &low_offset);
          set(ref(&offsets, c.~), offset);
        }
      }
    }

    // 2. Layout cells that _are_ part of the paramlist.  (They have to go beneath those which aren't.)
    pcells *shray[cell_num] = paramlist_cells(h->gr, gn);
    npcells size = count(pcells);
    for j size = npcells; j > 0; {
      j = j - 1;
      c cell_num = get(pcells, j);
      do_process bool;
      if isHas(ref(&offsets, c.~)) {
        // add_prim_fn_body applies primop directly to arg list.  TODO: Should we remove this special case here and make add_prim_fn_body copy the args into separate param cells?  Reconsider once we know how optimization would work.
        if case &GrPrimApply(a gr_prim_apply) = &ref_node(h->gr, gn)->op {
          do_process = false;
        } else {
          ice(_u8("cell is in paramlist, but already processed"));
        }
      } else {
        do_process = true;
      }

      if do_process {
        ci *cell_info = ref_cell(h->gr, c);
        offset cell_loc = next_stack_offset(h->cs, low_offset, ci, &low_offset);
        if !isNone(ref(&offsets, c.~)) {
          ice(_u8("cell is in paramlist, but already processed"));
        }
        set(&offsets, c.~, Has(offset));
      }
    }

    annotate(&annot->paramlist_bottom_frame_offset, low_offset);
  }

  // 3. Layout cells that aren't live during any function call -- maybe these are just used in dead code.
  ncells size = count(&offsets);
  for i size = 0; i < ncells; i = i + 1 {
    if isNone(ref(&offsets, i)) {
      c cell_num = ~i;
      ci *cell_info = ref_cell(h->gr, c);
      offset cell_loc = next_stack_offset(h->cs, low_offset, ci, &low_offset);
      set(&offsets, c.~, Has(offset));
    }
  }

  noopt_offsets array[cell_loc] = unHas_array(&offsets);
  annotate(&h->offsets, noopt_offsets);
  annotate(&h->low_offset, low_offset);
  return true;
}

// Props is the type properties of the return type.
func for_universal_cc_exists_hidden_return_param(plat *platform_info, props *type_properties) bool {
  switch plat->opsys {
  case Linux32(v void):
    if case IsScalarNo(v2 void) = props->is_scalar {
      return true;
    } else {
      return false;
    }
  case Win32(v void):
    n u32 = props->flat_size;
    if n <= 2 || n == 4 || n == 8 {
      if case &DerivedMethodTrivial(v2 void) = &props->move_behavior {
        return false;
      } else {
        return true;
      }
    } else {
      return true;
    }
  }
}

func next_stack_offset(cs *checkstate, low_offset i32, ci *cell_info, numeric_out *i32) cell_loc {
  switch ci->location {
  case LocationVirtual(v void):
    num i32 = floor_aligned(low_offset - ~cs->plat.ptrtraits.size, max(cs->plat.min_stackvar_alignment, cs->plat.ptrtraits.size));
    *numeric_out = num;
    return IndirectLoc(num);
  case LocationStatic(v void):
    num i32 = floor_aligned(low_offset - ~ci->props.flat_size, max(cs->plat.min_stackvar_alignment, ci->props.flat_alignment));
    *numeric_out = num;
    return DirectLoc(num);
  }
}


func dumbly_linearize_paramlist_ordering(h *frame, paramlist_ordering_out *array[gr_num]) bool {
  // Paramlists by in-degree, which is < the number of paramlists (since they can't point at themselves).
  nparamlists size = count(&h->bas->paramlists);
  // nparamlists is an impossible in-degree, and nparamlists + 1 is more impossible -- there's no way it could be decremented to zero while having count(&ordering) end up equalling nparamlists, below.
  backmap array[size] = repeat(count(&h->by_gn), nparamlists + 1);
  nreachable_paramlists size = 0;
  zeroset hash[gr_num, void];
  for i size = 0; i < nparamlists; i = i + 1 {
    gn gr_num = get(&h->bas->paramlists, i);
    // Nodes without lower_paramlists are unreachable.  So we don't need to worry about those!
    // TODO: Maybe, don't gen graphs with dead nodes!
    if case &Computed(arr array[gr_num]) = &ref(&h->by_gn, gn.~)->lower_paramlists {
      nlower size = count(unHas(&ref(&h->by_gn, gn.~)->lower_paramlists));
      if nlower == 0 {
        check_insert(&zeroset, &gn, void);
      }
      set(&backmap, gn.~, nlower);
      nreachable_paramlists = nreachable_paramlists + 1;
    }
  }

  ordering array[gr_num];

  while count(&zeroset) != 0 {
    gn gr_num;
    if true {
      var it = iter(&zeroset);
      gn = unHas(next(&it))->car;
    }
    check_remove(&zeroset, &gn);
    push(&ordering, gn);
    it hash_iter[gr_num, void] = iter(&ref_annot(h, gn)->higher_paramlists);
    while case Has(p *tup[gr_num, void]) = next(&it) {
      upn gr_num = p->car;
      bucket size = get(&backmap, upn.~);
      check(bucket > 0);
      nextbucket size = bucket - 1;
      if nextbucket == 0 {
        check_insert(&zeroset, &upn, void);
      }
      set(&backmap, upn.~, nextbucket);
    }
  }

  if count(&ordering) != nreachable_paramlists {
    ERR(_u8("ICE: paramlist ordering has a cyclic dependency."));
    return false;
  }

  *paramlist_ordering_out = ordering;
  return true;
}

func dumbly_gen_machine_code(f *objfile, h *frame, entry_gn gr_num) bool {
  if !dumbly_gen_function_intro(f, h) {
    return false;
  }
  if !dumbly_gen_function_body(f, h, entry_gn) {
    return false;
  }
  if !dumbly_gen_function_exit(f, h) {
    return false;
  }

  // TODO: Actually implement.
  return true;
}

func dumbly_gen_function_intro(f *objfile, h *frame) bool {
  switch h->cs->plat.callconv {
  case UniversalCdeclConvention(v void):
    x86_push32(f, X86_EBP);
    x86_mov_reg32(f, X86_EBP, X86_ESP);
    x86_add_esp_i32(f, *unHas(&h->low_offset));
    return true;
  }
  restore_esp_to_default_position(f, h);
}

func dumbly_gen_function_exit(f *objfile, h *frame) bool {
  switch h->cs->plat.callconv {
  case UniversalCdeclConvention(v void):
    ret_ci *cell_info = ref_cell(h->gr, h->return_cell);
    retsize u32 = ret_ci->props.flat_size;
    hrp bool = for_universal_cc_exists_hidden_return_param(&h->cs->plat, &ret_ci->props);
    if hrp {
      // Put hidden return pointer into eax.
      x86_load32(f, X86_EAX, X86_EBP, unIndirect(get_loc(h, h->return_cell)));
    } else {
      loc i32 = un_Direct(get_loc(h, h->return_cell));

      // TODO: Investigate calling convention behavior on signed types.  Should we sign-extend?  (Also a question for s1.)

      if retsize <= 4 {
        x86_load32_zeroextend(f, X86_EAX, X86_EBP, loc, retsize);
      } else if retsize <= 8 {
        // TODO: This should not even be theoretically reachable on Linux32.
        x86_load32(f, X86_EAX, X86_EBP, loc);
        x86_load32_zeroextend(f, X86_EDX, X86_EBP, loc + 4, retsize - 4);
      } else {
        ice(_u8("return size is too big for non-hidden return param"));
      }
    }

    x86_mov_reg32(f, X86_ESP, X86_EBP);
    x86_pop32(f, X86_EBP);
    switch h->cs->plat.opsys {
    case Linux32(v2 void):
      if hrp {
        x86_retn(f, ~ @[u32] 4);
      } else {
        x86_ret(f);
      }
    case Win32(v2 void):
      x86_ret(f);
    }
  }

  return true;
}

func universal_cc_write_to_retcell(f *objfile, h *frame, c cell_num) void {
  ci *cell_info = ref_cell(h->gr, c);
  retsize u32 = ci->props.flat_size;
  // TODO: The only time the retcell isn't static is in strinit and frame return_cell subcells.  Eventually we can use un_Direct here.
  dest_addr u8;
  dest_disp i32;
  x86_prep_loc_use(f, get_loc(h, c), X86_ECX, &dest_addr, &dest_disp);
  if retsize <= 4 {
    x86_store32_partial_destructively(f, dest_addr, dest_disp, X86_EAX, retsize);
  } else if retsize <= 8 {
    // TODO: This should not even be theoretically reachable on Linux32.
    x86_store32(f, dest_addr, dest_disp, X86_EAX);
    x86_store32_partial_destructively(f, dest_addr, dest_disp + 4, X86_EDX, retsize - 4);
  } else {
    ice(_u8("return size is too big for non-hidden return param"));
  }
}

func dumbly_gen_function_body(f *objfile, h *frame, entry_gn gr_num) bool {
  stack array[gr_stackent];
  push(&stack, EvalOpAndPrecs(entry_gn));
  while case Has(se gr_stackent) = popval(&stack) {
    switch se {
    case EvalOpAndPrecs(gn gr_num):
      annot *gn_annot = ref_annot(h, gn);
      if case &Computed(off u32) = &annot->instruction_offset {
        // TODO: Just gen the jmp directly -- no need for a placeholder, we know the offset.
        x86_gen_placeholder_jmp(f, h, gn);
      } else {
        annotate(&annot->instruction_offset, secsize(&f->text));
        push(&stack, EvalOp(gn));
        node *gr_node = ref_node(h->gr, gn);
        // I don't know why, it's just instinct, but on a whim for now we're going to push the stack such that we eval the precs left-to-right, just like we do in static eval.  This will mean differences in behavior in runtime versus static eval cannot be caused by divergence in prec evaluation ordering.
        nprecs size = count(&node->precs);
        for i size = nprecs; i > 0; {
          i = i - 1;
          push(&stack, EvalOpAndPrecs(get(&node->precs, i)));
        }
      }
    case EvalOp(gn gr_num):
      node *gr_node = ref_node(h->gr, gn);
      switch &node->op {
      case &GrApply(a gr_apply):
        switch h->cs->plat.callconv {
        case UniversalCdeclConvention(v void):
          ret_ci *cell_info = ref_cell(h->gr, a.retcell);
          hrp bool = for_universal_cc_exists_hidden_return_param(&h->cs->plat, &ret_ci->props);
          framelist_bottom i32 = *unHas(&ref_annot(h, gn)->paramlist_bottom_frame_offset);
          if hrp {
            hrp_bottom i32 = framelist_bottom - 4;
            x86_load_cell_address(f, h, X86_EAX, a.retcell);
            x86_store32(f, X86_EBP, hrp_bottom, X86_EAX);
            x86_load32(f, X86_EAX, X86_EBP, un_Direct(get_loc(h, a.funcell)));
            set_esp_to_frame_offset(f, h, hrp_bottom);
            x86_indirect_call(f, X86_EAX);
            restore_esp_to_default_position(f, h);
          } else {
            x86_load32(f, X86_EAX, X86_EBP, un_Direct(get_loc(h, a.funcell)));
            set_esp_to_frame_offset(f, h, framelist_bottom);
            x86_indirect_call(f, X86_EAX);
            universal_cc_write_to_retcell(f, h, a.retcell);
            restore_esp_to_default_position(f, h);
          }
        }
      case &GrPrimApply(a gr_prim_apply):
        if !x86_dumbly_prim_apply(f, h, &a) {
          return false;
        }
      case &GrMemCopy(a gr_memcopy):
        dest_ci *cell_info = ref_cell(h->gr, a.dest);
        src_ci *cell_info = ref_cell(h->gr, a.src);
        if dest_ci->props.flat_size != src_ci->props.flat_size {
          ice(_u8("GrMemCopy to have same-size cells."));
        }
        x86_memcopy(f, get_loc(h, a.dest), get_loc(h, a.src), dest_ci->props.flat_size);
      case &GrWriteConst(a gr_writeconst):
        dest_ci *cell_info = ref_cell(h->gr, a.dest);
        value st_value;
        if !st_const_compute_for_gen(h->cs, &a.value, dest_ci->props.flat_size, &value) {
          return false;
        }

        x86_gen_write_value(f, h, a.dest, &value);
      case &GrAddressof(a gr_addressof):
        x86_load_cell_address(f, h, X86_EAX, a.addressee);
        x86_store32(f, get_ptr_loc(h, a.dest), X86_EAX, X86_EDX);
      case &GrDeref(a gr_deref):
        x86_load32(f, X86_EAX, get_ptr_loc(h, a.pointer));
        x86_add_offset(f, h, X86_EAX, a.offset, X86_ECX, X86_EDX);
        x86_store32(f, X86_EBP, unIndirect(get_loc(h, a.name)), X86_EAX);
      case &GrSubcell(a gr_subcell):
        x86_load_cell_address(f, h, X86_EAX, a.partof);
        x86_add_offset(f, h, X86_EAX, a.offset, X86_ECX, X86_EDX);
        x86_store32(f, X86_EBP, unIndirect(get_loc(h, a.name)), X86_EAX);
      case &GrBranch(a gr_branch):
        // TODO: It'd be nice to codegen this without the gratuitous jmps.
        // TODO: Faster bool behavior.
        src_size u32 = ref_cell(h->gr, a.src)->props.flat_size;
        check(src_size == 4 || src_size == 2 || src_size == 1);
        src_addr u8;
        src_disp i32;
        x86_prep_loc_use(f, get_loc(h, a.src), X86_EAX, &src_addr, &src_disp);
        x86_load32_zeroextend(f, X86_EAX, src_addr, src_disp, src_size);
        ncases size = count(&a.cases);
        for i size = 0; i < ncases; i = i + 1 {
          cas *tup[gr_const, gr_num] = ref(&a.cases, i);
          expected st_value;
          if !st_const_compute_for_gen(h->cs, &cas->car, src_size, &expected) {
            return false;
          }
          x86_cmp_imm_sized(h->cs, f, X86_EAX, &expected, X86_ECX);
          x86_gen_placeholder_jcc(f, h, X86_JCC_Z, cas->cdr);
          push(&stack, EvalOpAndPrecs(cas->cdr));
        }
        if case Has(default_gn gr_num) = a.default_case {
          x86_gen_placeholder_jmp(f, h, default_gn);
          push(&stack, EvalOpAndPrecs(default_gn));
        } else {
          x86_gen_crash(f, h);
        }
      case &GrSequence(a gr_sequence):
        // first gets evaluated before second (because it gets pushed after)
        push(&stack, EvalOpAndPrecs(a.second));
        push(&stack, EvalOpAndPrecs(a.first));
      case &GrJmp(a gr_jmp):
        push(&stack, EvalOpAndPrecs(a.next));
      case &GrWriteNothing(a gr_writenothing):
        // Do nothing.  (This graph node is used for static eval, book keeping, etc.)
      case &GrDead(a gr_dead):
        // Do nothing.  (Likewise.)
      case &GrVirtualDead(a gr_virtual_dead):
        // Do nothing.  (Likewise.)
      case &GrNop(v void):
        // Do nothing (but do it less glamorously).
      }
    }
  }

  njumps size = count(&h->placeholder_jump_fillin);
  for i size = 0; i < njumps; i = i + 1 {
    p *jmpdata = ref(&h->placeholder_jump_fillin, i);
    target u32 = *unHas(&ref_annot(h, p->target)->instruction_offset);
    diff i32 = @[i32]~target - @[i32]~p->relative_mathpoint;
    le le_i32 = ~diff;
    overwrite_raw(&f->text, ~p->overwrite_location, &le.buf[0], 4);
  }

  ncrash_jumps size = count(&h->crash_jumps);
  if ncrash_jumps > 0 {
    target u32 = secsize(&f->text);
    x86_int_3(f);

    for i size = 0; i < ncrash_jumps; i = i + 1 {
      p *crash_jmpdata = ref(&h->crash_jumps, i);
      diff i32 = @[i32]~target - @[i32]~p->relative_mathpoint;
      le le_i32 = ~diff;
      overwrite_raw(&f->text, ~p->overwrite_location, &le.buf[0], 4);
    }
  }

  return true;
}

func x86_load_cell_address(f *objfile, h *frame, dest u8, c cell_num) void {
  switch get_loc(h, c) {
  case DirectLoc(off i32):
    x86_lea32(f, X86_EAX, X86_EBP, off);
  case IndirectLoc(off i32):
    x86_load32(f, X86_EAX, X86_EBP, off);
  }
}

func x86_add_offset(f *objfile, h *frame, dest u8, offset gr_offset, scratch1 u8, scratch2 u8) void {
  switch offset {
  case OffsetConst(off u32):
    if off != 0 {
      x86_mov_imm32(f, scratch1, ~off);
      x86_add_w32(f, dest, scratch1);
    }
  case OffsetComputed(factors tup[u32, cell_num]):
    x86_load32(f, scratch1, get_size_loc(h, factors.cdr));
    x86_mov_imm32(f, scratch2, ~factors.car);
    x86_imul_w32(f, scratch1, scratch2);
    x86_add_w32(f, dest, scratch1);
  }
}

func x86_gen_placeholder_jmp(f *objfile, h *frame, target gr_num) void {
  offset u32 = 1 + secsize(&f->text);
  b [5]u8;
  b[0] = 0xE9;
  b[1] = 0;
  b[2] = 0;
  b[3] = 0;
  b[4] = 0;
  append_raw(&f->text, &b[0], 5);
  push(&h->placeholder_jump_fillin, {offset, offset + 4, target});
}

func x86_gen_placeholder_jcc(f *objfile, h *frame, x86_jcc_code u8, target gr_num) void {
  offset u32 = 2 + secsize(&f->text);
  b [6]u8;
  b[0] = 0x0F;
  b[1] = x86_jcc_code;
  b[2] = 0;
  b[3] = 0;
  b[4] = 0;
  b[5] = 0;
  append_raw(&f->text, &b[0], 6);
  push(&h->placeholder_jump_fillin, {offset, offset + 4, target});
}

func x86_gen_crash_jcc(f *objfile, h *frame, x86_jcc_code u8) void {
  offset u32 = 2 + secsize(&f->text);
  b [6]u8;
  b[0] = 0x0F;
  b[1] = x86_jcc_code;
  b[2] = 0;
  b[3] = 0;
  b[4] = 0;
  b[5] = 0;
  append_raw(&f->text, &b[0], 6);
  push(&h->crash_jumps, {offset, offset + 4});
}

func x86_gen_crash(f *objfile, h *frame) void {
  x86_int_3(f);
}

func set_esp_to_frame_offset(f *objfile, h *frame, offset i32) void {
  x86_lea32(f, X86_ESP, X86_EBP, offset);
}

func restore_esp_to_default_position(f *objfile, h *frame) void {
  set_esp_to_frame_offset(f, h, *unHas(&h->low_offset));
}

func x86_gen_write_value(f *objfile, h *frame, dest cell_num, value *st_value) void {
  check(count(&value->objrefs) == 0);
  check(h->cs->plat.bnno_size == 8);

  len u32 = value->length;
  if len == 0 {
    return;
  }

  srefs array[tup[u32, st_symbolref]] = value->symbolrefs;
  sort(&srefs);

  dest_addr u8;
  dest_disp i32;
  x86_prep_loc_use(f, get_loc(h, dest), X86_EAX, &dest_addr, &dest_disp);

  w u32 = 0;
  nsrefs size = count(&srefs);
  for i size = 0; i < nsrefs; i = i + 1 {
    sref *tup[u32, st_symbolref] = ref(&srefs, i);
    check(w <= sref->car);
    x86_gen_write_data(f, h, dest_addr, dest_disp, &value->datas, w, sref->car);
    symbol_table_index sti = *unHas(&ref_fn_body(h->cs, sref->cdr.fnid)->symbol_table_index);
    x86_mov_stiptr(f, X86_ECX, symbol_table_index);
    check(st_symbolref_size(&h->cs->plat, &sref->cdr) == 4);
    x86_store32(f, dest_addr, dest_disp + ~sref->car, X86_ECX);
    w = sref->car + 4;
  }

  x86_gen_write_data(f, h, dest_addr, dest_disp, &value->datas, w, len);
}

func x86_gen_write_data(f *objfile, h *frame, dest_addr u8, dest_disp i32, datas *array[u32], begin u32, end u32) void {
  check(begin <= end);
  if begin == end {
    return;
  }

  // TODO: We assume (in terms of the optimality of this code) that dest_addr+dest_disp is aligned modulo 4.

  i u32 = begin;
  if i / 4 < end / 4 {
    if i % 2 == 1 {
      bval u8 = ~(0xFF & (get(datas, ~(i / 4)) >> ((i % 4) * 8)));
      // TODO: Seriously, store the imm8 directly.
      x86_store_imm8(f, dest_addr, dest_disp + ~i, bval);
      i = i + 1;
    }
    if i % 4 == 2 {
      wval u16 = ~(get(datas, ~(i / 4)) >> 16);
      x86_store_imm16(f, dest_addr, dest_disp + ~i, wval);
      i = i + 2;
    }
  }
  emod u32 = end % 4;
  loend u32 = end - emod;
  while i < loend {
    dval u32 = get(datas, ~(i / 4));
    x86_store_imm32(f, dest_addr, dest_disp + ~i, dval);
    i = i + 4;
  }

  if i == end {
    return;
  }
  last u32 = get(datas, ~(i / 4));
  if emod >= 2 {
    x86_store_imm16(f, dest_addr, dest_disp + ~i, @[u16]~(last & 0xFFFF));
    last = last >> 16;
    i = i + 2;
  }
  if i < end {
    x86_store_imm8(f, dest_addr, dest_disp + ~i, @[u8]~(last & 0xFF));
    i = i + 1;
  }
  check(i == end);
}

func x86_cmp_imm_sized(cs *checkstate, f *objfile, lhs u8, rhs *st_value, avail u8) void {
  if count(&rhs->symbolrefs) != 0 {
    check(count(&rhs->symbolrefs) == 1);
    check(rhs->length == 4);
    srefpair tup[u32, st_symbolref] = get(&rhs->symbolrefs, 0);
    check(srefpair.car == 0);
    sref st_symbolref = srefpair.cdr;
    check(st_symbolref_size(&cs->plat, &sref) == 4);
    x86_mov_stiptr(f, avail, *unHas(&ref_fn_body(cs, sref.fnid)->symbol_table_index));
    x86_cmp_w32(f, lhs, avail);
  } else {
    val u32 = get(&rhs->datas, 0);
    if rhs->length == 1 {
      check(x86_reg32_has_lobyte(lhs));
      x86_cmp_reg8_imm8(f, lhs, ~val);
    } else if rhs->length == 2 {
      x86_cmp_reg16_imm16(f, lhs, ~val);
    } else if rhs->length == 4 {
      x86_cmp_imm32(f, lhs, val);
    } else {
      ice(_u8("x86_cmp_imm_sized bad length"));
    }
  }
}

func `<`(x tup[u32, st_symbolref], y tup[u32, st_symbolref]) bool {
  return x.car < y.car;
}

func x86_dumbly_prim_apply(f *objfile, h *frame, a *gr_prim_apply) bool {
  switch &a->primop {
  case &PrimNum(b primitive_numeric_op):
    arity u32 = op_arity(b.op_action);
    check(count(&a->params) == ~arity);
    x86_load32_zeroextend(f, X86_EAX, get_loc(h, get(&a->params, 0)), b.op_size);
    if arity == 2 {
      x86_load32_zeroextend(f, X86_ECX, get_loc(h, get(&a->params, 1)), b.op_size);
    } else {
      check(arity == 1);
    }

    switch b.op_action {
    case NumAdd(v void):
      if b.op_size == 1 {
        x86_add_w8(f, X86_EAX, X86_ECX);
      } else if b.op_size == 2 {
        x86_add_w16(f, X86_EAX, X86_ECX);
      } else if b.op_size == 4 {
        x86_add_w32(f, X86_EAX, X86_ECX);
      } else {
        ice(_u8("NumAdd bad size"));
      }
      x86_gen_crash_jcc(f, h, x86_repr_overflow_code(b.op_numtraits.repr));
    case NumSub(v void):
      if b.op_size == 1 {
        x86_sub_w8(f, X86_EAX, X86_ECX);
      } else if b.op_size == 2 {
        x86_sub_w16(f, X86_EAX, X86_ECX);
      } else if b.op_size == 4 {
        x86_sub_w32(f, X86_EAX, X86_ECX);
      } else {
        ice(_u8("NumSub bad size"));
      }
      x86_gen_crash_jcc(f, h, x86_repr_overflow_code(b.op_numtraits.repr));
    case NumMul(v void):
      if b.op_size == 1 {
        switch b.op_numtraits.repr {
        case Unsigned(v2 void):
          x86_alah_mul_w8(f, X86_ECX);
          x86_gen_crash_jcc(f, h, X86_JCC_C);
          x86_movzx8_reg8(f, X86_EAX, X86_EAX);
        case SignedTwos(v2 void):
          x86_alah_imul_w8(f, X86_ECX);
          x86_gen_crash_jcc(f, h, X86_JCC_O);
          x86_movzx8_reg8(f, X86_EAX, X86_EAX);
        }
      } else if b.op_size == 2 {
        switch b.op_numtraits.repr {
        case Unsigned(v2 void):
          x86_dxax_mul_w16(f, X86_ECX);
          x86_gen_crash_jcc(f, h, X86_JCC_C);
        case SignedTwos(v2 void):
          x86_imul_w16(f, X86_EAX, X86_ECX);
          x86_gen_crash_jcc(f, h, X86_JCC_O);
        }
      } else if b.op_size == 4 {
        switch b.op_numtraits.repr {
        case Unsigned(v2 void):
          x86_eaxedx_mul_w32(f, X86_ECX);
          x86_gen_crash_jcc(f, h, X86_JCC_C);
        case SignedTwos(v2 void):
          x86_imul_w32(f, X86_EAX, X86_ECX);
          x86_gen_crash_jcc(f, h, X86_JCC_O);
        }
      } else {
        ice(_u8("NumMul bad size"));
      }
    case NumDiv(v void):
      if b.op_size == 1 {
        switch b.op_numtraits.repr {
        case Unsigned(v2 void):
          x86_alah_div_w8(f, X86_ECX);
          // Divide by zero will produce #DE.
          // Wipe out modulus in AH.
          x86_movzx8_reg8(f, X86_EAX, X86_EAX);
        case SignedTwos(v2 void):
          x86_alah_idiv_w8(f, X86_ECX);
          // Divide by zero, -128/-1 (I guess) will produce #DE.
          // Wipe out modulus in AH (don't sign-extend?).
          x86_movzx8_reg8(f, X86_EAX, X86_EAX);
        }
      } else if b.op_size == 2 {
        switch b.op_numtraits.repr {
        case Unsigned(v2 void):
          x86_xor_w32(f, X86_EDX, X86_EDX);
          x86_axdx_div_w16(f, X86_ECX);
          // Divide by zeroe will produce #DE.
        case SignedTwos(v2 void):
          x86_cwd_w16(f);
          x86_axdx_idiv_w16(f, X86_ECX);
          // Divide by zero, INT16_MIN/-1 will produce #DE.
        }
      } else if b.op_size == 4 {
        switch b.op_numtraits.repr {
        case Unsigned(v2 void):
          x86_xor_w32(f, X86_EDX, X86_EDX);
          x86_eaxedx_div_w32(f, X86_ECX);
          // Divide by zero will produce #DE.
        case SignedTwos(v2 void):
          x86_cdq_w32(f);
          x86_eaxedx_idiv_w32(f, X86_ECX);
          // Divide by zero, INT32_MIN / -1 will produce #DE.
        }
      } else {
        ice(_u8("NumDiv bad size"));
      }
    default:
      return TODO();
    }

    // (TODO: Write EAX into destination.)
    return TODO();

  case &PrimNumCompare(b primitive_numeric_comparison_op):
    return TODO();
  case &PrimLogical(b primitive_logical_op):
    return TODO();
  case &PrimPtrCompare(b primitive_ptr_comparison_op):
    return TODO();
  case &PrimConversion(b primitive_conversion_op):
    return TODO();
  }
}

func x86_repr_overflow_code(repr numeric_representation) u8 {
  switch repr {
  case Unsigned(v void):
    return X86_JCC_C;
  case SignedTwos(v void):
    return X86_JCC_O;
  }
}
