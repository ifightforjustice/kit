import dumbanal;
import state;

defenum cell_loc {
  // Directly via an frame offset, per se.
  DirectLoc i32;
  // Indirectly via a pointer (through the frame offset).
  IndirectLoc i32;
};

deftype lives struct {
  all hash[cell_num, void];
};

deftype gn_annot struct {
  // Which cells are guaranteed to be live when the gn starts.
  live nc[lives];

  // Only for GrApply and GrPrimApply.
  lower_paramlists nc[array[gr_num]];
  higher_paramlists hash[gr_num, void];
};

deftype frame struct {
  cs *checkstate;
  im *identmap;
  argcells array[cell_num];
  return_cell cell_num;
  gr *frame_graph;
  bas *basic_analyze_state;
  // by_gn[i].fs shows what the state must is _before_ evaluating node i.  (All incoming nodes must agree!)
  by_gn array[gn_annot];
  // offsets[i] is the offset of a cell relative to the stack frame, or location of a cell, or something like that.
  offsets nc[array[cell_loc]];
  low_offset nc[i32];
};

func mk_gn_annot() gn_annot {
  return {NotComputed(), NotComputed(), mk_hash@[gr_num, void]()};
}

func unIndirect(loc cell_loc) i32 {
  if case IndirectLoc(x i32) = loc {
    return x;
  } else {
    ice(_u8("unIndirect fail"));
    return fake();
  }
}

func un_Direct(loc cell_loc) i32 {
  if case DirectLoc(x i32) = loc {
    return x;
  } else {
    ice(_u8("un_Direct fail"));
    return fake();
  }
}

func get_loc(h *frame, c cell_num) cell_loc {
  return get(unHas(&h->offsets), c.~);
}

// Some Hungarian notation:  The "dumbly_" prefix means the code's really dumb.

func dumbly_gen_graphed_fn_body(cs *checkstate, f *objfile, bas *basic_analyze_state, g *graphed_fn_body) bool {
  h frame = {cs, cs->im, g->argcells, g->graph.cell, &g->graph.gr, bas, repeat(count(&g->graph.gr.ops), mk_gn_annot()), NotComputed(), NotComputed@[i32]()};

  prelive lives;
  check_add_all(&prelive.all, &g->argcells);
  check_add(&prelive.all, g->graph.cell);

  DBG(_u8("gen graphed fn body for "), lookup(cs->im, g->graph.gr.informal_name));
  mut genexpr_result;
  if !dumbly_analyze_expression(&h, &g->graph.gr, g->graph.gn, &prelive, &mut) {
    return false;
  }

  switch &mut {
  case &Terminal(m exprmut):
    new_live lives = prelive;
    compose_live(&new_live, &m);
    expected lives = {mk_hash(g->graph.cell, void)};
    if !lives_equal(&new_live, &expected) {
      ice(_u8("fn_body leaves cells alive, expected "), g->graph.cell, _u8(", saw "), new_live);
    }
  case &NonTerminal(pm partial_exprmut):
    // nothing to assert
  }

  if !dumbly_layout_frame_cells(g, &h) {
    return false;
  }

  if !dumbly_gen_machine_code(f, &h, g->graph.gn) {
    return false;
  }

  return true;
}

func paramlist_cells(gr *frame_graph, gn gr_num) *shray[cell_num] {
  switch &ref_node(gr, gn)->op {
  case &GrApply(a gr_apply):
    return &a.params;
  case &GrPrimApply(a gr_prim_apply):
    return &a.params;
  default:
    ice(_u8("paramlist_cells called on non-paramlist op"));
    return fake();
  }
}

func dumbly_layout_frame_cells(g *graphed_fn_body, h *frame) bool {
  // Left-most paramlist goes first.
  paramlist_ordering array[gr_num];
  if !dumbly_linearize_paramlist_ordering(h, &paramlist_ordering) {
    return false;
  }

  // Offsets of either a static cell or a virtual cell's pointer.
  offsets array[opt[cell_loc]] = repeat(count(&h->gr->cells), None());

  // TODO: Maybe after we push the frame pointer, the alignment is 4 or something -- in cases where we need 8-byte or 16-byte alignment for some things.
  low_offset i32 = 0;

  // 0. Layout arg cells, return cell.
  switch h->cs->plat.callconv {
  case UniversalCdeclConvention(v void):
    // It's OK to hard-code numbers here, because we're under the bullshit "UniversalCDeclConvention".
    upward_offset i32 = 8;
    if for_universal_cc_exists_hidden_return_param(h) {
      set(ref(&offsets, h->return_cell.~), IndirectLoc(upward_offset));
      upward_offset = upward_offset + 4;
    } else {
      // Put its cell below the frame pointer.
      ret_ci *cell_info = ref_cell(h->gr, h->return_cell);
      offset cell_loc = next_stack_offset(h->cs, low_offset, ret_ci, &low_offset);
      set(ref(&offsets, h->return_cell.~), offset);
    }

    nargcells size = count(&h->argcells);
    for i size = 0; i < nargcells; i = i + 1 {
      c cell_num = get(&h->argcells, i);
      ci *cell_info = ref_cell(h->gr, c);
      if ci->location != LocationStatic(void) {
        ice(_u8("Non-static arglist cell"));
      }
      offset i32 = upward_offset;
      upward_offset = ceil_aligned(offset + ~ci->props.flat_size, 4);
      set(ref(&offsets, c.~), DirectLoc(offset));
    }
  }


  ncalls size = count(&paramlist_ordering);
  for i size = 0; i < ncalls; i = i + 1 {
    gn gr_num = get(&paramlist_ordering, i);
    live *lives = unHas(&ref_annot(h, gn)->live);

    // 1. Layout any cells live during the funcall, that are _not_ part of the paramlist.
    it hash_iter[cell_num, void] = iter(&live->all);
    while case Has(p *tup[cell_num, void]) = next(&it) {
      c cell_num = p->car;
      if isNone(ref(&offsets, c.~)) {
        if case &Has(cell_paramlistgn gr_num) = &ref(&h->bas->celldisp, c.~)->paramlist {
          // Cells in other paramlists should already been processed -- and would have failed the isNone check just above.
          if cell_paramlistgn == gn {
            ice(_u8("cell is in other paramlist, was not processed"));
          }
        } else {
          ci *cell_info = ref_cell(h->gr, c);
          offset cell_loc = next_stack_offset(h->cs, low_offset, ci, &low_offset);
          set(ref(&offsets, c.~), offset);
        }
      }
    }

    // 2. Layout cells that _are_ part of the paramlist.  (They have to go beneath those which aren't.)
    pcells *shray[cell_num] = paramlist_cells(h->gr, gn);
    npcells size = count(pcells);
    for j size = npcells; j > 0; {
      j = j - 1;
      c cell_num = get(pcells, j);
      do_process bool;
      if isHas(ref(&offsets, c.~)) {
        // add_prim_fn_body applies primop directly to arg list.  TODO: Should we remove this special case here and make add_prim_fn_body copy the args into separate param cells?  Reconsider once we know how optimization would work.
        if case &GrPrimApply(a gr_prim_apply) = &ref_node(h->gr, gn)->op {
          do_process = false;
        } else {
          ice(_u8("cell is in paramlist, but already processed"));
        }
      } else {
        do_process = true;
      }

      if do_process {
        ci *cell_info = ref_cell(h->gr, c);
        offset cell_loc = next_stack_offset(h->cs, low_offset, ci, &low_offset);
        if !isNone(ref(&offsets, c.~)) {
          ice(_u8("cell is in paramlist, but already processed"));
        }
        set(&offsets, c.~, Has(offset));
      }
    }
  }

  // 3. Layout cells that aren't live during any function call -- maybe these are just used in dead code.
  ncells size = count(&offsets);
  for i size = 0; i < ncells; i = i + 1 {
    if isNone(ref(&offsets, i)) {
      c cell_num = ~i;
      ci *cell_info = ref_cell(h->gr, c);
      offset cell_loc = next_stack_offset(h->cs, low_offset, ci, &low_offset);
      set(&offsets, c.~, Has(offset));
    }
  }

  noopt_offsets array[cell_loc] = unHas_array(&offsets);
  annotate(&h->offsets, noopt_offsets);
  annotate(&h->low_offset, low_offset);
  return true;
}

func for_universal_cc_exists_hidden_return_param(h *frame) bool {
  return for_universal_cc_exists_hidden_return_param(&h->cs->plat, &ref_cell(h->gr, h->return_cell)->props);
}

func for_universal_cc_exists_hidden_return_param(plat *platform_info, props *type_properties) bool {
  switch plat->opsys {
  case Linux32(v void):
    if case IsScalarNo(v2 void) = props->is_scalar {
      return true;
    } else {
      return false;
    }
  case Win32(v void):
    n u32 = props->flat_size;
    if n <= 2 || n == 4 || n == 8 {
      if case &DerivedMethodTrivial(v2 void) = &props->move_behavior {
        return false;
      } else {
        return true;
      }
    } else {
      return true;
    }
  }
}

func next_stack_offset(cs *checkstate, low_offset i32, ci *cell_info, numeric_out *i32) cell_loc {
  switch ci->location {
  case LocationVirtual(v void):
    num i32 = floor_aligned(low_offset - ~cs->plat.ptrtraits.size, max(cs->plat.min_stackvar_alignment, cs->plat.ptrtraits.size));
    *numeric_out = num;
    return IndirectLoc(num);
  case LocationStatic(v void):
    num i32 = floor_aligned(low_offset - ~ci->props.flat_size, max(cs->plat.min_stackvar_alignment, ci->props.flat_alignment));
    *numeric_out = num;
    return DirectLoc(num);
  }
}


func dumbly_linearize_paramlist_ordering(h *frame, paramlist_ordering_out *array[gr_num]) bool {
  // Paramlists by in-degree, which is < the number of paramlists (since they can't point at themselves).
  nparamlists size = count(&h->bas->paramlists);
  // nparamlists is an impossible in-degree, and nparamlists + 1 is more impossible -- there's no way it could be decremented to zero while having count(&ordering) end up equalling nparamlists, below.
  backmap array[size] = repeat(count(&h->by_gn), nparamlists + 1);
  nreachable_paramlists size = 0;
  zeroset hash[gr_num, void];
  for i size = 0; i < nparamlists; i = i + 1 {
    gn gr_num = get(&h->bas->paramlists, i);
    // Nodes without lower_paramlists are unreachable.  So we don't need to worry about those!
    // TODO: Maybe, don't gen graphs with dead nodes!
    if case &Computed(arr array[gr_num]) = &ref(&h->by_gn, gn.~)->lower_paramlists {
      nlower size = count(unHas(&ref(&h->by_gn, gn.~)->lower_paramlists));
      if nlower == 0 {
        check_insert(&zeroset, &gn, void);
      }
      set(&backmap, gn.~, nlower);
      nreachable_paramlists = nreachable_paramlists + 1;
    }
  }

  ordering array[gr_num];

  while count(&zeroset) != 0 {
    gn gr_num;
    if true {
      var it = iter(&zeroset);
      gn = unHas(next(&it))->car;
    }
    check_remove(&zeroset, &gn);
    push(&ordering, gn);
    it hash_iter[gr_num, void] = iter(&ref_annot(h, gn)->higher_paramlists);
    while case Has(p *tup[gr_num, void]) = next(&it) {
      upn gr_num = p->car;
      bucket size = get(&backmap, upn.~);
      check(bucket > 0);
      nextbucket size = bucket - 1;
      if nextbucket == 0 {
        check_insert(&zeroset, &upn, void);
      }
      set(&backmap, upn.~, nextbucket);
    }
  }

  if count(&ordering) != nreachable_paramlists {
    ERR(_u8("ICE: paramlist ordering has a cyclic dependency."));
    return false;
  }

  *paramlist_ordering_out = ordering;
  return true;
}

func dumbly_gen_machine_code(f *objfile, h *frame, entry_gn gr_num) bool {
  if !dumbly_gen_function_intro(f, h) {
    return false;
  }
  if !dumbly_gen_function_body(f, h, entry_gn) {
    return false;
  }
  if !dumbly_gen_function_exit(f, h) {
    return false;
  }

  // TODO: Actually implement.
  return true;
}

func dumbly_gen_function_intro(f *objfile, h *frame) bool {
  switch h->cs->plat.callconv {
  case UniversalCdeclConvention(v void):
    x86_push32(f, X86_EBP);
    x86_mov_reg32(f, X86_EBP, X86_ESP);
    x86_add_esp_i32(f, *unHas(&h->low_offset));
    return true;
  }
}

func dumbly_gen_function_body(f *objfile, h *frame, entry_gn gr_num) bool {
  return TODO();
}

func dumbly_gen_function_exit(f *objfile, h *frame) bool {
  switch h->cs->plat.callconv {
  case UniversalCdeclConvention(v void):
    ret_ci *cell_info = ref_cell(h->gr, h->return_cell);
    retsize u32 = ret_ci->props.flat_size;
    hrp bool = for_universal_cc_exists_hidden_return_param(h);
    if hrp {
      // Put hidden return pointer into eax.
      x86_load32(f, X86_EAX, X86_EBP, unIndirect(get_loc(h, h->return_cell)));
    } else {
      loc i32 = un_Direct(get_loc(h, h->return_cell));

      // TODO: Investigate calling convention behavior on signed types.  Should we sign-extend?  (Also a question for s1.)

      if retsize <= 4 {
        x86_load32_zeroextend(f, loc, retsize);
      } else if retsize <= 8 {
        // TODO: This should not even be theoretically reachable on Linux32.
        x86_load32(f, X86_EAX, X86_EBP, loc);
        x86_load32_zeroextend(f, loc, retsize - 4);
      } else {
        ice(_u8("return size is too big for non-hidden return param"));
      }
    }
    return TODO();
  }
}